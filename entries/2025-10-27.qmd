---
title: "2025-10-27"
format:
  html: default
  pdf: default
params:
  course: "mc501"
  word_min: 450
  word_max: 500
  p1: 'Think about a media environment you engage with regularly—TikTok, news headlines, TV dramas, YouTube comments, etc. Choose one and describe a research question that could be answered through content analysis. What would you want to measure? Would you be more interested in manifest content (what’s there) or latent content (the underlying tone or message), and why?'
 
---

## Choose **one** prompt to answer

> **Prompt A:** `r params$p1`

---

## Response

<!-- RESPONSE-START -->
For this week’s journal entry, I must self-identify as an eternal Reddit lurker. I’ve never once created an account or posted a comment. I share many posts with friends, have my favorite Subreddits and read more comments that I’d like to admit. There is no doubt that my doomscrolling on Reddit has had a negative impact on my mental health. I have lost literally hours of sleep by scrolling at length before bedtime. In the current political climate, I do not have a positive view of my fellow US citizens who are continuing to support the Trump regime. The blatant disregard for the law and shitposting of Trump’s social media is maddening. While I feel helpless to change anything, at least I try to stay on top of the news. I acknowledge that my main news source is Reddit and how problematic that may be. I should also admit that most of the time I’m not reading past the headline, but I do read the comments under the post.

With that being said, I have to imagine there are millions of other American Reddit users who share the same disdain for MAGA Republicans. Looking at this from a research perspective, I believe Reddit threads are ripe for analysis. I think for this hypothetical research it would be appropriate to further define what the research is looking at. Angry comments carry a certain weight, but I would want to look at posts that pertain to actual policy decisions. This is opposed to the endless “liberal activating” social media posts of Trump flying a jet or ICE being in Halo. There are bonafide executive orders, lawsuits and votes on policy that have impacts on the American people. I would frame my research question as: “How is anger expressed and directed in left-leaning political discussions on Reddit in response to conservative policy proposals?”

With regards to looking at manifest content or latent content, I believe there is a deeper later anger in the comments of the Reddit posts I’d theoretically be looking at. Given the nuance and overwhelming quantity of comments that need to be parsed through it would be important to establish a system to sort out which comments have more depth than “this sucks” or “ F Trump”. I believe that there should be an estimated coding system that could measure the intensity of the anger in any given post. It would be important to also establish a digest of emotion words and assign a scale to how angry they may represent the poster’s sentiment. Also, many of the comments I see under political posts are lengthy and may need some human control to understand the intensity of the sentiment. In the end, the research would need to be triangulated to combine quantitative scores with human judgments to confirm the detection of anger’s latent intensity. I would forecast that posts about racial justice contain higher latent anger intensity than posts about environmental policy.
<!-- RESPONSE-END -->

---

## Word Count & Range Check

```{r}
#| echo: false
#| message: false
#| warning: false
get_response_text <- function() {
  f <- knitr::current_input()
  if (is.null(f) || !file.exists(f)) return("")
  x <- readLines(f, warn = FALSE)
  # Find the lines that EXACTLY match the start/end markers
  s <- grep("^<!-- RESPONSE-START -->$", x)
  e <- grep("^<!-- RESPONSE-END -->$", x)
  if (length(s) != 1 || length(e) != 1 || e <= s) return("")
  paste(x[(s + 1L):(e - 1L)], collapse = "\n")
}
count_words <- function(txt) {
  # Remove code blocks and inline code before counting
  txt <- gsub("```[\\s\\S]*?```", " ", txt, perl = TRUE)
  txt <- gsub("`[^`]*`", " ", txt, perl = TRUE)
  # Keep letters, numbers, spaces, hyphens, and apostrophes
  txt <- gsub("[^\\p{L}\\p{N}\\s'-]", " ", txt, perl = TRUE)
  # Split by whitespace and count non-empty words
  words <- unlist(strsplit(txt, "\\s+", perl = TRUE))
  words <- words[nzchar(words)]
  length(words)
}
txt <- get_response_text()
n <- count_words(txt)
minw <- as.integer(params$word_min)
maxw <- as.integer(params$word_max)
in_range <- n >= minw && n <= maxw
cat(sprintf("**Word count:** %d  \n", n))
cat(sprintf("**Required range (%s):** %d–%d words  \n",
            toupper(params$course), minw, maxw))
cat(if (in_range) "**Status:** ✅ In range\n" else "**Status:** ❌ Out of range\n")
```
